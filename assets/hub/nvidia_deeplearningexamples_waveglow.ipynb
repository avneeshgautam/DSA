{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a76ef331",
      "metadata": {
        "id": "a76ef331"
      },
      "source": [
        "### This notebook requires a GPU runtime to run.\n",
        "### Please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# WaveGlow\n",
        "\n",
        "*Author: NVIDIA*\n",
        "\n",
        "**WaveGlow model for generating speech from mel spectrograms (generated by Tacotron2)**\n",
        "\n",
        "<img src=\"https://pytorch.org/assets/images/waveglow_diagram.png\" alt=\"alt\" width=\"50%\"/>\n",
        "\n",
        "\n",
        "\n",
        "### Model Description\n",
        "\n",
        "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables user to synthesise a natural sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model (also available via torch.hub) produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech.\n",
        "\n",
        "### Example\n",
        "\n",
        "In the example below:\n",
        "- pretrained Tacotron2 and Waveglow models are loaded from torch.hub\n",
        "- Given a tensor representation of the input text (\"Hello world, I missed you so much\"), Tacotron2 generates a Mel spectrogram as shown on the illustration\n",
        "- Waveglow generates sound given the mel spectrogram\n",
        "- the output sound is saved in an 'audio.wav' file\n",
        "\n",
        "To run the example you need some extra python packages installed.\n",
        "These are needed for preprocessing the text and audio, as well as for display and input / output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d1c66900",
      "metadata": {
        "id": "d1c66900",
        "outputId": "4914f003-cd15-4d98-9c5b-d3e22b8c6fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install numpy scipy librosa unidecode inflect librosa\n",
        "apt-get update\n",
        "apt-get install -y libsndfile1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307d6ab2",
      "metadata": {
        "id": "307d6ab2"
      },
      "source": [
        "Load the WaveGlow model pre-trained on [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "98cc03dd",
      "metadata": {
        "id": "98cc03dd",
        "outputId": "961cfda0-8d23-4987-d262-bed8f9b660f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/waveglow/entrypoints.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(ckpt_file)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553411ef",
      "metadata": {
        "id": "553411ef"
      },
      "source": [
        "Prepare the WaveGlow model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2ab7546",
      "metadata": {
        "id": "c2ab7546",
        "outputId": "555002c0-9459-4194-ed0a-5943d609ec3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WaveGlow(\n",
              "  (upsample): ConvTranspose1d(80, 80, kernel_size=(1024,), stride=(256,))\n",
              "  (WN): ModuleList(\n",
              "    (0-3): 4 x WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (4-7): 4 x WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (8-11): 4 x WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "  )\n",
              "  (convinv): ModuleList(\n",
              "    (0-3): 4 x Invertible1x1Conv(\n",
              "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (4-7): 4 x Invertible1x1Conv(\n",
              "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (8-11): 4 x Invertible1x1Conv(\n",
              "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e96d0aa",
      "metadata": {
        "id": "4e96d0aa"
      },
      "source": [
        "Load a pretrained Tacotron2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287a3fbd",
      "metadata": {
        "id": "287a3fbd"
      },
      "outputs": [],
      "source": [
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')\n",
        "tacotron2 = tacotron2.to('cuda')\n",
        "tacotron2.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9a6d9a",
      "metadata": {
        "id": "eb9a6d9a"
      },
      "source": [
        "Now, let's make the model say:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b64d22",
      "metadata": {
        "id": "b3b64d22"
      },
      "outputs": [],
      "source": [
        "text = \"hello world, I missed you so much\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d71967e",
      "metadata": {
        "id": "1d71967e"
      },
      "source": [
        "Format the input using utility methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc464c1",
      "metadata": {
        "id": "6bc464c1"
      },
      "outputs": [],
      "source": [
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
        "sequences, lengths = utils.prepare_input_sequence([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb969c93",
      "metadata": {
        "id": "bb969c93"
      },
      "source": [
        "Run the chained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0409068c",
      "metadata": {
        "id": "0409068c"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "    audio = waveglow.infer(mel)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22050"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e8c877",
      "metadata": {
        "id": "76e8c877"
      },
      "source": [
        "You can write it to a file and listen to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27f72b9",
      "metadata": {
        "id": "d27f72b9"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import write\n",
        "write(\"audio.wav\", rate, audio_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b58b6238",
      "metadata": {
        "id": "b58b6238"
      },
      "source": [
        "Alternatively, play it right away in a notebook with IPython widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3c57dd",
      "metadata": {
        "id": "2e3c57dd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "Audio(audio_numpy, rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18168ee",
      "metadata": {
        "id": "f18168ee"
      },
      "source": [
        "### Details\n",
        "For detailed information on model input and output, training recipies, inference and performance visit: [github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2) and/or [NGC](https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch)\n",
        "\n",
        "### References\n",
        "\n",
        " - [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)\n",
        " - [WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/abs/1811.00002)\n",
        " - [Tacotron2 and WaveGlow on NGC](https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch)\n",
        " - [Tacotron2 and Waveglow on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}